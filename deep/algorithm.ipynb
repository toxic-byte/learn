{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68e63d4",
   "metadata": {},
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ef43e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "x=np.array([2.0,1.0,0.1])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20883d",
   "metadata": {},
   "source": [
    "# BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42811724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_norm(x,alpha,gamma,epsilon=1e-12):\n",
    "    x_mean=np.mean(x,axis=0)\n",
    "    x_var=np.var(x,axis=0) #按列计算方差\n",
    "\n",
    "    x_hat=(x-x_mean)/np.sqrt(x_var+epsilon)\n",
    "    out=gamma*x_hat+alpha #线性变换\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661ccd5",
   "metadata": {},
   "source": [
    "# LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81456b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def layer_norm(x,alpha,gamma,epsilon=1e-12):\n",
    "    x_mean=np.mean(x,axis=1,keepdims=True) #按行计算均值\n",
    "    x_var=np.var(x,axis=1,keepdims=True) #按行计算方差\n",
    "\n",
    "    x_hat=(x-x_mean)/np.sqrt(x_var+epsilon)\n",
    "    out=gamma*x_hat+alpha #线性变换\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e0e36",
   "metadata": {},
   "source": [
    "# 对比损失：正样本距离尽可能小，负样本距离尽可能大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.norm(output1 - output2, dim=1)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccf897",
   "metadata": {},
   "source": [
    "# 双塔里pairwise的margin loss:正负样本之间的距离尽可能大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PairwiseMarginLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(PairwiseMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_distance = torch.norm(anchor - positive, dim=1)\n",
    "        neg_distance = torch.norm(anchor - negative, dim=1)\n",
    "        loss = torch.mean(torch.clamp(pos_distance - neg_distance + self.margin, min=0.0))\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15807c96",
   "metadata": {},
   "source": [
    "# 交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d971e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2797765622367497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets * np.log(predictions + 1e-9)) / N\n",
    "    return ce\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]])\n",
    "print(cross_entropy(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2320e1a",
   "metadata": {},
   "source": [
    "# focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b38ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003109072441720451\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def focal_loss(predictions, targets, alpha=0.25, gamma=2.0, epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    pt = np.where(targets == 1, predictions, 1 - predictions)\n",
    "    loss = -alpha * (1 - pt) ** gamma * np.log(pt)\n",
    "    return np.mean(loss)\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]])\n",
    "print(focal_loss(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25382dd6",
   "metadata": {},
   "source": [
    "# IOU交并比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b421feb",
   "metadata": {},
   "source": [
    "# multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (self.head_dim * num_heads == embed_size), \"Embedding size needs to be divisible by num_heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf94199",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70974cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(w,x,y,lr=0.001,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "    m=len(y)\n",
    "    mt=np.zeros_like(w)\n",
    "    vt=np.zeros_like(w)\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        i=np.random.randint(0,m)\n",
    "        xi=x[i,:]\n",
    "        yi=y[i]\n",
    "\n",
    "        grad=xi.reshape(-1,1)*(np.dot(xi,w)-yi)\n",
    "        t+=1\n",
    "\n",
    "        mt=beta1*mt+(1-beta1)*grad\n",
    "        vt=beta2*vt+(1-beta2)*(grad**2)\n",
    "\n",
    "        mt_hat=mt/(1-beta1**t)\n",
    "        vt_hat=vt/(1-beta2**t)\n",
    "\n",
    "        w=w-lr*mt_hat/(np.sqrt(vt_hat)+epsilon)#用修正后的动量（mt_hat和 vt_hat）更新模型参数 w。\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00ede0",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_scores):\n",
    "    pos_indices = np.where(y_true == 1)[0]\n",
    "    neg_indices = np.where(y_true == 0)[0]\n",
    "    \n",
    "    n_pos=len(pos_indices)\n",
    "    n_neg=len(neg_indices)\n",
    "    \n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "    \n",
    "    count= 0\n",
    "    for i in pos_indices:\n",
    "        for j in neg_indices:\n",
    "            if y_scores[i] > y_scores[j]:\n",
    "                count += 1\n",
    "            elif y_scores[i] == y_scores[j]:\n",
    "                count += 0.5        \n",
    "    \n",
    "    auc_value = count / (n_pos * n_neg)\n",
    "    return auc_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1c0c8",
   "metadata": {},
   "source": [
    "# bpr loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import sigmoid\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    loss = -np.mean(np.log(sigmoid(pos_scores - neg_scores)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5bcf6",
   "metadata": {},
   "source": [
    "# 手撕线性回归的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd404ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据\n",
    "np.random.seed(0)\n",
    "X=2*np.random.rand(100,1)\n",
    "y=4+3*X+np.random.randn(100,1)\n",
    "\n",
    "#超参数\n",
    "learning_rate=0.01\n",
    "n_iterations=1000\n",
    "m=len(y)\n",
    "\n",
    "w=np.random.randn(1,1) #初始化权重\n",
    "b=0.0 #初始化偏置\n",
    "\n",
    "#梯度下降\n",
    "for epoch in range(n_iterations):\n",
    "    y_pred=X.dot(w)+b\n",
    "    error=y_pred-y#误差\n",
    "\n",
    "    #计算梯度\n",
    "    dw=2/m*X.T.dot(error)\n",
    "    db=2/m*np.sum(error)\n",
    "\n",
    "    #更新参数\n",
    "    w=w-learning_rate*dw\n",
    "    b=b-learning_rate*db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb122c83",
   "metadata": {},
   "source": [
    "# 手撕LR的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_loss(x,y,theta):\n",
    "    m=len(y)\n",
    "    y_pred=sigmoid(np.dot(x,theta))\n",
    "    loss=-1/m*(np.dot(y.T,np.log(y_pred))+np.dot((1-y).T,np.log(1-y_pred)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(x,y,theta):\n",
    "    m=len(y)\n",
    "    y_pred=sigmoid(np.dot(x,theta))\n",
    "    gradient=1/m*np.dot(x.T,(y_pred-y))\n",
    "    return gradient\n",
    "\n",
    "#添加截距项\n",
    "def add_intercept(x):\n",
    "    intercept=np.ones((x.shape[0],1))\n",
    "    return np.concatenate((intercept,x),axis=1)\n",
    "\n",
    "def gradient_descent(x,y,learning_rate=0.01,n_iterations=1000):\n",
    "    loss_history=[]\n",
    "    for i in range(n_iterations):\n",
    "        gradient=compute_gradient(x,y,theta)\n",
    "        theta=theta-learning_rate*gradient\n",
    "        loss=compute_loss(x,y,theta)\n",
    "        loss_history.append(loss)\n",
    "    return theta,loss_history\n",
    "\n",
    "def predict(x,theta,threshold=0.5):\n",
    "    x=add_intercept(x)\n",
    "    probs=sigmoid(np.dot(x,theta))\n",
    "    return (probs>=threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8eec6",
   "metadata": {},
   "source": [
    "# 自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54271bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __intt__(self,d_model,d_k,d_v):\n",
    "        super(SingleHeadSelfAttention,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_k=d_k\n",
    "        self.d_v=d_v\n",
    "\n",
    "        self.W_q=nn.Linear(d_model,d_k)\n",
    "        self.W_k=nn.Linear(d_model,d_k)\n",
    "        self.W_v=nn.Linear(d_model,d_v)\n",
    "\n",
    "        self.fc=nn.Linear(d_v,d_model)\n",
    "\n",
    "    def forward(self,queries,keys,values,mask=None):\n",
    "        Q=self.W_q(queries)\n",
    "        K=self.W_k(keys)\n",
    "        V=self.W_v(values)\n",
    "\n",
    "        scores=torch.matmul(Q,K.transpose(-2,-1))/torch.sqrt(torch.tensor(self.d_k,dtype=torch.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores=scores.masked_fill(mask==0,float('-1e9'))\n",
    "\n",
    "        attention_weights=F.softmax(scores,dim=-1)\n",
    "        out=torch.matmul(attention_weights,V)\n",
    "        out=self.fc(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
